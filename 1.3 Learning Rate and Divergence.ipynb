{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate and Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch: https://www.youtube.com/watch?v=Fn8qXpIcdnI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understand how the learning rate influences the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"files/old.png\"> **Same Code** used in 1.2 Using Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1.],[2.],[3.]])\n",
    "y = tf.constant([[3.],[5.],[7.]])\n",
    "\n",
    "weight = tf.Variable([[0.]])\n",
    "bias = tf.Variable([[0.]])\n",
    "\n",
    "y_pred = tf.matmul(x, weight) + bias\n",
    "\n",
    "loss = (y - y_pred)**2\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the learning rate vs loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"files/new.png\"> **New Code:** Loop over learning rates between 0.003 and 0.073 and check the total error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003 10.1312\n",
      "0.006 0.968732\n",
      "0.008 0.173809\n",
      "0.011 0.0128408\n",
      "0.013 0.00460723\n",
      "0.016 0.00348134\n",
      "0.018 0.00336204\n",
      "0.021 0.00321676\n",
      "0.023 0.00312381\n",
      "0.026 0.00298922\n",
      "0.028 0.00290257\n",
      "0.031 0.00277702\n",
      "0.033 0.00269622\n",
      "0.036 0.00257918\n",
      "0.038 0.00250387\n",
      "0.041 0.00239494\n",
      "0.043 0.00232863\n",
      "0.046 0.00248484\n",
      "0.048 0.00493796\n",
      "0.051 0.0632202\n",
      "0.053 0.380786\n",
      "0.056 4.42218\n",
      "0.058 19.5145\n",
      "0.061 149.925\n",
      "0.063 524.505\n",
      "0.066 2996.39\n",
      "0.068 8848.04\n",
      "0.071 40520.4\n",
      "0.073 105160.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,30):   \n",
    "  \n",
    "    #the fancy formula to calculate learning rate is to ensure that the table bellow alignes properly\n",
    "    learning_rate = int(i*1000/400 +1)/1000.\n",
    "    \n",
    "    #reset the variables to default values\n",
    "    sess.run(weight.assign([[0.]]))\n",
    "    sess.run(bias.assign([[0.]]))\n",
    "    \n",
    "    #define the train_step with the new learning_rate\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #take 10 steps\n",
    "    for j in range(0,10):\n",
    "        sess.run(train_step)\n",
    "        \n",
    "    print learning_rate, sess.run((loss[0] + loss[1] + loss[2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For this particular function, if the learning rate is too small (bellow 0.01), gradient descent takes too many steps to find the minimum and fails to find it in 10 steps.\n",
    "\n",
    "On the other hand, if the learning rate is too large (higher than 0.05), the gradient descent will overshoot and diverge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
